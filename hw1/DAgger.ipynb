{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is written by Mathias Rose Bjare (muthissar@gmail.com) and  answears Deep RL Assignment 1, http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw1.pdf.\n",
    "The code base for this file can be found at https://github.com/muthissar/homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from behavioral_cloning import *\n",
    "from load_policy import load_policy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAGGER\n",
    "Implimentation of the dagger algorithm. The algoirthm is run on the 'Hopper-v2' task, for which behaviroal cloning failed to perform as well as expert. The setup is as identical to the setup for previous section. The network is identical, the algorithm is initialized with the same number of expert rollouts (20). About 10 iterations is needed to perform as good as the expert. At each iteration 20 rollouts are made, and the return is computed and stored. This is printed in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 11) (1, 11)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "expert_rollout = 20\n",
    "dagger_rollout = 5\n",
    "#0 = run until termination\n",
    "max_timesteps = 0\n",
    "use_expert_cache = True\n",
    "store=False\n",
    "training_epochs = 1\n",
    "losses = []\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.reset_default_graph()\n",
    "losses = []\n",
    "\n",
    "it_dagger = 20\n",
    "#tasks = ['Hopper-v2', 'Ant-v2', 'HalfCheetah-v2',\n",
    "# 'Humanoid-v2', 'Reacher-v2','Walker2d-v2']\n",
    "tasks = ['Hopper-v2']\n",
    "results = pd.DataFrame(columns=[])\n",
    "for task in tasks:\n",
    "    tf.reset_default_graph()\n",
    "    tf_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n",
    "            #device_count = {'GPU': 2},\n",
    "            intra_op_parallelism_threads=1,\n",
    "            gpu_options = tf.GPUOptions(\n",
    "                #per_process_gpu_memory_fraction=1./16. # 1gb\n",
    "                allow_growth=True\n",
    "                )\n",
    "        )\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        \n",
    "        expert_policy = load_policy('./experts/{}.pkl'.format(task))\n",
    "        if use_expert_cache:\n",
    "            dataset, expert_returns = get_dataset(\\\n",
    "                './expert_data/{}-{}.pkl'.format(task,expert_rollout))\n",
    "        else:\n",
    "            dataset, expert_returns = get_dataset(\\\n",
    "                envname=task,render=False,\n",
    "                expert_policy_file='experts/'+task+'.pkl'\\\n",
    "                ,max_timesteps=0,num_rollouts=expert_rollout,store=store)\n",
    "        dataset = dataset.map(lambda x, y: (tf.cast(x,tf.float32),y))\n",
    "        obs_dim = dataset.output_shapes[0].as_list()\n",
    "        action_dim = dataset.output_shapes[1].as_list()[1:]\n",
    "        dataset = dataset.repeat(training_epochs)\n",
    "        batched_dataset = dataset.batch(batch_size)\n",
    "        iterator = batched_dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "        # FNN network\n",
    "        behavioral_net = BehavioralCloningNet(obs_dim, action_dim,\\\n",
    "            batch_size,learning_rate=0.001)\n",
    "        losses = []\n",
    "        returns = []\n",
    "        # object used for interacting with gym environment\n",
    "        evaluate = EvaluateDagger(task,expert_policy,behavioral_net,render=False)\n",
    "        saver = tf.train.Saver()\n",
    "        for i in range(it_dagger):\n",
    "            if i % 5 == 0:\n",
    "                print(\"iiiiii \" + str(i))\n",
    "                saver.save(sess,'dagger_model{}-{}.ckpt'.format(task,i))\n",
    "            losses += behavioral_net.train(next_element)\n",
    "            evaluate.expert_obs = []\n",
    "            evaluate.expert_actions = []\n",
    "            video_path = \"{}_{}\".format(task,i) if i % 10 == 0 else None\n",
    "            #render = True if i % 10 == 0 else False\n",
    "            render = False\n",
    "            return_ = evaluate.evaluate(rollouts=dagger_rollout,video_path=video_path,render=render)\n",
    "            print(\"dagger it {}, mean return {}\".format(i,np.mean(return_)))\n",
    "            returns.append(return_)\n",
    "            expert_obs_arr = np.asarray(evaluate.expert_obs,dtype=np.float32)\n",
    "            expert_actions_arr = np.asarray(evaluate.expert_actions,\\\n",
    "                dtype=np.float32)\n",
    "            expert_labeled_new_data = tf.data.Dataset.from_tensor_slices(\\\n",
    "                (expert_obs_arr,expert_actions_arr))\n",
    "            dataset = dataset.concatenate(expert_labeled_new_data)\n",
    "            dataset = dataset.shuffle(buffer_size=10000)\n",
    "            # avoid that the dataset explodes eponentially by having\n",
    "            #a cutoff for which the dataset will then grow linearly.\n",
    "            dataset = dataset.take(10000)\n",
    "            batched_dataset = dataset.batch(batch_size,True)\n",
    "            iterator = batched_dataset.make_one_shot_iterator()\n",
    "            next_element = iterator.get_next()\n",
    "            results = results.append({'Task': task,\n",
    "                                'mean return': np.mean(return_),\n",
    "                                'std return': np.std(return_),\n",
    "                                'expert mean return': np.mean(expert_returns),\n",
    "                                'expert std return': np.std(expert_returns),\n",
    "                                'expert rollouts': expert_rollout,\n",
    "                                'training epochs': training_epochs,\n",
    "                                'dagger its': i},\n",
    "                                ignore_index=True)\n",
    "#fig = plt.figure()\n",
    "#plt.plot(losses)\n",
    "#plt.xlabel('Batch')\n",
    "#plt.ylabel('Loss (mean square)')\n",
    "#plt.legend(['Train loss {}'.format(task)],loc='upper center')\n",
    "#plt.show(fig)\n",
    "results.to_csv('results_dagger.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly run from cached results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('results_dagger.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve\n",
    "Learning curve is plotted for different dagger iterations. Also the experts return is plotted. It is seen at some steps that the variance of the 20 rollouts are high, and that the convergence to the experts return is jumpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "hopper_data = results[results['Task']=='Hopper-v2']\n",
    "dagger_its = hopper_data['dagger its'].values\n",
    "plt.errorbar(dagger_its,hopper_data['mean return'].values,\\\n",
    "    yerr=[hopper_data['std return'], hopper_data['std return'],], c='blue',fmt='-o',linestyle=\":\")\n",
    "plt.errorbar(dagger_its,hopper_data['expert mean return'].values,\\\n",
    "    yerr=[hopper_data['expert std return'], hopper_data['expert std return']\\\n",
    "    ,], c='red')\n",
    "plt.xlabel(\"Dagger iterations\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend(['Hopper','Hopper expert'])\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 11) (1, 11)\n",
      "INFO:tensorflow:Restoring parameters from dagger_modelHopper-v2-10.ckpt\n",
      "Creating window glfw\n",
      "Tensor(\"behavioral_cloning/fully_connected_1/BiasAdd:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from behavioral_cloning import *\n",
    "from load_policy import load_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "expert_policy = load_policy('./experts/{}.pkl'.format('Hopper-v2'))\n",
    "dataset, expert_returns = get_dataset(\\\n",
    "        './expert_data/{}-{}.pkl'.format('Hopper-v2',20))\n",
    "dataset = dataset.map(lambda x, y: (tf.cast(x,tf.float32),y))\n",
    "obs_dim = dataset.output_shapes[0].as_list()\n",
    "action_dim = dataset.output_shapes[1].as_list()[1:]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n",
    "        device_count = {'GPU': 0},\n",
    "        #intra_op_parallelism_threads=1,\n",
    "        #gpu_options = tf.GPUOptions(\n",
    "            #per_process_gpu_memory_fraction=1./16. # 1gb\n",
    "        #    allow_growth=True\n",
    "        #    )\n",
    "    )\n",
    "with tf.Session(config=tf_config) as sess:\n",
    "    policy_estimator = BehavioralCloningNet(obs_dim,action_dim, 1)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    #sess = tf.get_default_session()\n",
    "    env = gym.make('Hopper-v2')\n",
    "    saver = tf.train.Saver()\n",
    "    model = 'dagger_modelHopper-v2-10.ckpt'\n",
    "    saver.restore(sess,model)\n",
    "    state = env.reset()\n",
    "    env.render('human')\n",
    "    viewer = env.env.viewer                \n",
    "    #HIDE CONTROLS\n",
    "    viewer.key_callback(None,glfw.KEY_H,None,glfw.RELEASE,None)\n",
    "    #PAUSE \n",
    "    #viewer.key_callback(None,glfw.KEY_SPACE,None,glfw.RELEASE,None)\n",
    "    #FOLLOW AGENT\n",
    "    #viewer.key_callback(None,glfw.KEY_TAB,None,glfw.RELEASE,None)\n",
    "    #ZOOM OUT\n",
    "    viewer.move_camera(const.MOUSE_ZOOM, 0, -0.05 * 30)\n",
    "    #viewer.move_camera(const.MOUSE_ROTATE_H, 0, -0.05 * 20)\n",
    "    #PLAY SLOWER \n",
    "    for i in range(3):\n",
    "        viewer.key_callback(None,glfw.KEY_S,None,glfw.RELEASE,None)\n",
    "    \n",
    "    #observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    print(policy_estimator.output_layer)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        env.render('human')\n",
    "        action = policy_estimator.predict([state])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    viewer = None\n",
    "env.close()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\lstinputlisting[language=Python]{behavioral_cloning.py}"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
