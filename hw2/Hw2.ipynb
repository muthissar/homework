{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
    "import os\n",
    "try:\n",
    "\tos.chdir(os.path.join(os.getcwd(), '..'))\n",
    "\tprint(os.getcwd())\n",
    "except:\n",
    "\tpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.chdir(os.path.join(os.getcwd(), 'course_berkeley/homework/hw2'))\n",
    "    print(os.getcwd())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Problem 1\n",
    " We want to show that\n",
    " \\begin{equation}\n",
    " \\sum_{t'=1}^{T}\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[\\nabla_{\\theta}log \\pi_{\\theta}(a_{t'}|s_{t'})(b(s_{t'}))]=0\n",
    " \\end{equation}\n",
    " \\begin{align}\n",
    " \\sum_{t=1}^{T}\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)(b(s_t))]\n",
    " &=\\sum_{t=1}^{T}\\int p_{\\theta}(\\tau)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)(b(s_t))d\\tau\\\\\n",
    " %&=\\sum_{t=1}^{T}\\int p_{\\theta}(s_t,a_t)p_{\\theta}(\\tau / s_t,a_t\\mid s_t,a_t)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)(b(s_t))d\\tau\\\\\n",
    " &=\\sum_{t=1}^{T}\\int \\int p_{\\theta}(s_t,a_t)(\\int p_{\\theta}(\\tau / s_t,a_t\\mid s_t,a_t)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)(b(s_t))d\\tau )d s_t da_t)\\\\\n",
    " &=\\sum_{t=1}^{T}\\int \\int p_{\\theta}(s_t,a_t)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)b(s_t)\\int p_{\\theta}(\\tau / s_t,a_t\\mid s_t,a_t)d\\tau d s_t d a_t\\\\\n",
    " &=\\sum_{t=1}^{T}\\int \\int p_{\\theta}(s_t,a_t)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)b(s_t) d s_t d a_t\\\\\n",
    " &=\\sum_{t=1}^{T}\\int \\int p(s_{t+1}|a_t,s_t)\\pi_{\\theta}(a_t|s_t)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)b(s_t) d s_t d a_t\\\\\n",
    " &=\\sum_{t=1}^{T}\\int \\int p(s_{t+1}|a_t,s_t)b(s_t)\\nabla_{\\theta}\\pi_{\\theta}(a_t|s_t) d s_t d a_t\\\\\n",
    " &=\\sum_{t=1}^{T} \\nabla_{\\theta} \\int \\int p(s_{t+1}|a_t,s_t)b(s_t)\\pi_{\\theta}(a_t|s_t) d s_t d a_t \\\\\n",
    " &=\\sum_{t=1}^{T} \\nabla_{\\theta} \\int \\int p_{\\theta}(a_t,s_t)b(s_t) d s_t d a_t \\\\\n",
    " \\end{align}\n",
    "Where $\\nabla_{\\theta}log \\pi_{\\theta}(A_t|S_t)b(S_t)=b(S_t)\\nabla_{\\theta}log \\pi_{\\theta}(A_t|S_t)$, since $b$ is assumed not to depend on $\\theta$ (in practice it could be estimated using a different neural network with parameters $\\phi$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  &=\\mathbb{E}_{(A_t,S_t)}[\\mathbb{E}_{\\tau / (A_t,S_t) \\sim p_{\\theta}(\\tau / S_t,A_t\\mid S_t,A_t)}[0\\mid A_t , S_t]]\\\\\n",
    "  &= \\mathbf{0}\n",
    " Where $\\nabla_{\\theta}log \\pi_{\\theta}(A_t|S_t)(b(S_t)) = 0$ given $A_t$ and $S_t$. Therefore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_pg_f18 import train_PG\n",
    "seed = 1\n",
    "train_PG(\n",
    "                exp_name=\"sb_no_rtg_dna\",\n",
    "                env_name=\"CartPole-v0\",\n",
    "                n_iter=100,\n",
    "                gamma=0.99,\n",
    "                min_timesteps_per_batch=1000,\n",
    "\t\t\t\tlearning_rate=0.001,\n",
    "\t\t\t\treward_to_go=None,\n",
    "                max_path_length=99999999999,\n",
    "                animate=False,\n",
    "                logdir= None , #os.path.join(\"data\",'%d'%seed),\n",
    "\t\t\t\tnormalize_advantages=False,\n",
    "                nn_baseline=None, \n",
    "                seed=seed,\n",
    "                n_layers=2,\n",
    "                size=100\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test continous  actions\n",
    "We use the logprob expression\n",
    "```\n",
    "sy_logprob_n =  -1/2 * tf.reduce_sum(inverse_variance *(diff*diff),axis=1)\n",
    "```\n",
    "Even though\n",
    "```\n",
    "sy_logprob_n = - 1/2*(tf.log(variance)  + tf.reduce_sum(inverse_variance *(diff*diff),axis=1))\n",
    "```\n",
    "would have been more correct as to follow the logprob. For this small example this makes the variance be maximized after, and thus gives more incorrect results, however for the real examples it performes the same, probably due to maximization errors when doing the other thing. It is also what is suggested in Levines slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[0.],\n",
      "       [0.],\n",
      "       [0.]], dtype=float32), array([0.], dtype=float32)), array([-2., -2., -2.], dtype=float32), 2.0]\n",
      "std: 0.9857619404792786, mean: 0.053708259016275406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/30/0/70339/.local/lib/python3.6/site-packages/matplotlib/figure.py:445: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 99, std: 4.66080379486084, mean: 5.440565586090088\n",
      "it: 199, std: 6.097592830657959, mean: 4.187251091003418\n",
      "it: 299, std: 44.20418167114258, mean: 9.711440086364746\n",
      "it: 399, std: 111.15580749511719, mean: -4.385540962219238\n",
      "it: 499, std: 151.47842407226562, mean: 27.827970504760742\n",
      "it: 599, std: 186.75601196289062, mean: -14.619833946228027\n",
      "it: 699, std: 214.3723907470703, mean: 23.425809860229492\n",
      "it: 799, std: 224.56509399414062, mean: -21.883249282836914\n",
      "it: 899, std: 266.2421875, mean: -15.566503524780273\n",
      "it: 999, std: 271.6304626464844, mean: 19.704458236694336\n",
      "it: 1099, std: 304.3190002441406, mean: 26.475332260131836\n",
      "it: 1199, std: 300.62518310546875, mean: 10.948735237121582\n",
      "it: 1299, std: 336.2844543457031, mean: 23.704082489013672\n",
      "it: 1399, std: 375.9537353515625, mean: 68.61624145507812\n",
      "it: 1499, std: 345.3172302246094, mean: 39.9020881652832\n",
      "it: 1599, std: 362.7534484863281, mean: 26.962749481201172\n",
      "it: 1699, std: 387.88238525390625, mean: -39.475982666015625\n",
      "it: 1799, std: 368.72686767578125, mean: 40.964393615722656\n",
      "it: 1899, std: 415.8846130371094, mean: -0.6535653471946716\n",
      "it: 1999, std: 399.17840576171875, mean: -9.46063232421875\n",
      "it: 2099, std: 402.3943176269531, mean: 56.75299835205078\n",
      "it: 2199, std: 426.382568359375, mean: -9.584866523742676\n",
      "it: 2299, std: 397.1225280761719, mean: 32.09247589111328\n",
      "it: 2399, std: 440.0720520019531, mean: 11.018421173095703\n",
      "it: 2499, std: 448.4039306640625, mean: -24.780982971191406\n",
      "it: 2599, std: 431.1240539550781, mean: -5.091939926147461\n",
      "it: 2699, std: 454.5281066894531, mean: 127.94454193115234\n",
      "it: 2799, std: 522.6281127929688, mean: -42.318817138671875\n",
      "it: 2899, std: 539.570068359375, mean: 48.2517204284668\n",
      "it: 2999, std: 541.5744018554688, mean: -17.586069107055664\n",
      "it: 3099, std: 556.2981567382812, mean: 21.580507278442383\n",
      "it: 3199, std: 577.2489013671875, mean: 22.744112014770508\n",
      "it: 3299, std: 580.4326171875, mean: -20.103782653808594\n",
      "it: 3399, std: 575.3218994140625, mean: 100.09042358398438\n",
      "it: 3499, std: 584.818603515625, mean: 65.43984985351562\n",
      "it: 3599, std: 532.0647583007812, mean: 60.5954704284668\n",
      "it: 3699, std: 582.7791748046875, mean: -98.67958068847656\n",
      "it: 3799, std: 574.6396484375, mean: -18.537797927856445\n",
      "it: 3899, std: 621.84228515625, mean: 28.96257781982422\n",
      "it: 3999, std: 598.8465576171875, mean: -39.415287017822266\n"
     ]
    }
   ],
   "source": [
    "from train_pg_f18 import train_PG, Agent\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "computation_graph_args = {\n",
    "    'n_layers': 1,\n",
    "    'ob_dim': 3,\n",
    "    'ac_dim': 1,\n",
    "    'discrete': False,\n",
    "    'size': 2,\n",
    "    'learning_rate': 0.005,\n",
    "}\n",
    "\n",
    "sample_trajectory_args = {\n",
    "    'animate': False,\n",
    "    'max_path_length': 1000,\n",
    "    'min_timesteps_per_batch': 1000,\n",
    "}\n",
    "\n",
    "estimate_return_args = {\n",
    "    'gamma': 0.99,\n",
    "    'reward_to_go': True,\n",
    "    'nn_baseline': False,\n",
    "    'normalize_advantages': False,\n",
    "}\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    agent = Agent(computation_graph_args, sample_trajectory_args, estimate_return_args)\n",
    "    agent.build_computation_graph()\n",
    "    tf_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n",
    "        intra_op_parallelism_threads=1,\n",
    "        gpu_options = tf.GPUOptions(\n",
    "            #per_process_gpu_memory_fraction=1./16. # 1gb\n",
    "            allow_growth=True\n",
    "        )\n",
    "    )\n",
    "    with tf.Session(config=tf_config).as_default() as sess:\n",
    "        \n",
    "\n",
    "        sess.__enter__() # equivalent to `with self.sess:`\n",
    "        tf.global_variables_initializer().run() #pylint: disable=E1101\n",
    "\n",
    "\n",
    "        # check that the loss is computed correct\n",
    "        print(sess.run([agent.policy_parameters, agent.sy_logprob_n, agent.loss], {\n",
    "            agent.sy_ob_no: [[0.5,0.5,0.5],\n",
    "                            [0.5,0.5,0.5],\n",
    "                            [0.5,0.5,0.5]],\n",
    "            agent.sy_ac_na: [[2.0], [2.0], [2.0]],\n",
    "            agent.sy_adv_n: [1,1,1]\n",
    "        \n",
    "        })\n",
    "        )\n",
    "        # check that the actions look correct\n",
    "        n_samples = 100\n",
    "        sampled_actions = sess.run(agent.sy_sampled_ac, {\n",
    "            agent.sy_ob_no: [[0.5,0.5,0.5]]*n_samples,\n",
    "            #agent.sy_ac_na: [[2.0]]*n_samples,\n",
    "            #agent.sy_adv_n: [1]*n_samples\n",
    "        \n",
    "        })\n",
    "        figure =  plt.figure()\n",
    "        plot = plt.hist(sampled_actions)\n",
    "        figure.show()\n",
    "        print(\"std: {}, mean: {}\".format(np.std(sampled_actions),np.mean(sampled_actions)))\n",
    "        n_training = 4000\n",
    "        for i in range(n_training):\n",
    "            sy_ac_na = np.random.normal(loc=5.0 , scale=0.1,size=n_samples)\n",
    "            sess.run(agent.update_op, {\n",
    "                agent.sy_ob_no: [[0.5,0.5,0.5]]*n_samples,\n",
    "                agent.sy_ac_na: sy_ac_na.reshape(-1,1),\n",
    "                agent.sy_adv_n: [1]*n_samples\n",
    "\n",
    "            })\n",
    "            sampled_actions = sess.run(agent.sy_sampled_ac, {\n",
    "                agent.sy_ob_no: [[0.5,0.5,0.5]]*n_samples,\n",
    "                #agent.sy_ac_na: [[2.0]]*n_samples,\n",
    "                #agent.sy_adv_n: [1]*n_samples\n",
    "            })\n",
    "            if i %100 == 99:\n",
    "                print(\"it: {}, std: {}, mean: {}\".format(i, np.std(sampled_actions),np.mean(sampled_actions)))\n",
    "        \n",
    "        figure =  plt.figure()\n",
    "        plot = plt.hist(sampled_actions)\n",
    "        figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of state before [0. 0.]\n",
      "Target values [ 1. -1.]\n",
      "Number of training baseline: 1000\n",
      "Value of state after [ 0.9999992 -0.9999991]\n"
     ]
    }
   ],
   "source": [
    "from train_pg_f18 import train_PG, Agent\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "computation_graph_args = {\n",
    "    'n_layers': 1,\n",
    "    'ob_dim': 3,\n",
    "    'ac_dim': 1,\n",
    "    'discrete': False,\n",
    "    'size': 2,\n",
    "    'learning_rate': 0.005,\n",
    "}\n",
    "\n",
    "sample_trajectory_args = {\n",
    "    'animate': False,\n",
    "    'max_path_length': 1000,\n",
    "    'min_timesteps_per_batch': 1000,\n",
    "}\n",
    "\n",
    "estimate_return_args = {\n",
    "    'gamma': 0.99,\n",
    "    'reward_to_go': True,\n",
    "    'nn_baseline': True,\n",
    "    'normalize_advantages': False,\n",
    "}\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    agent = Agent(computation_graph_args, sample_trajectory_args, estimate_return_args)\n",
    "    agent.build_computation_graph()\n",
    "    tf_config = tf.ConfigProto(inter_op_parallelism_threads=1,\n",
    "        intra_op_parallelism_threads=1,\n",
    "        gpu_options = tf.GPUOptions(\n",
    "            #per_process_gpu_memory_fraction=1./16. # 1gb\n",
    "            allow_growth=True\n",
    "        )\n",
    "    )\n",
    "    with tf.Session(config=tf_config).as_default() as sess:\n",
    "        \n",
    "\n",
    "        sess.__enter__() # equivalent to `with sess:`\n",
    "        tf.global_variables_initializer().run() #pylint: disable=E1101\n",
    "\n",
    "        # check that the actions look correct\n",
    "        n_samples = 1\n",
    "        ob_no =  [[0.0,0.0,0.0],[-1.,-1.,-1.]]*n_samples\n",
    "        q_n = [1,-1]*n_samples\n",
    "        value = sess.run(agent.baseline_prediction, {\n",
    "                agent.sy_ob_no: ob_no\n",
    "            })\n",
    "        print(\"Value of state before {}\".format(value))\n",
    "        target_n = (q_n - np.mean(q_n))/np.std(q_n)\n",
    "        print(\"Target values {}\".format(target_n))\n",
    "        n_training = 1000\n",
    "        print(\"Number of training baseline: {}\".format(n_training))\n",
    "        for _ in range(n_training):\n",
    "            sess.run(agent.baseline_update_op, {\n",
    "                agent.sy_ob_no: ob_no,\n",
    "                agent.sy_target_n: target_n\n",
    "            })\n",
    "            value = sess.run(agent.baseline_prediction, {\n",
    "                    agent.sy_ob_no: ob_no\n",
    "                })\n",
    "        print(\"Value of state after {}\".format(value))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
